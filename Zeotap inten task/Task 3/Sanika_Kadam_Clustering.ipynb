{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMERS_PATH = \"Customers.csv\"\n",
    "TRANSACTIONS_PATH = \"Transactions.csv\"\n",
    "PRODUCTS_PATH = \"Products.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    customers = pd.read_csv(CUSTOMERS_PATH)\n",
    "    transactions = pd.read_csv(TRANSACTIONS_PATH)\n",
    "    products = pd.read_csv(PRODUCTS_PATH)\n",
    "        \n",
    "    print(\"Data loaded successfully!\")\n",
    "    return customers, transactions, products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_features(customers, transactions, products):\n",
    "    customers['SignupDate'] = pd.to_datetime(customers['SignupDate'])\n",
    "    transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate'])\n",
    "        \n",
    "    le = LabelEncoder()\n",
    "    customers['Region_encoded'] = le.fit_transform(customers['Region'])\n",
    "        \n",
    "    customer_metrics = transactions.groupby('CustomerID').agg({\n",
    "            'TransactionID': 'count',\n",
    "            'TotalValue': ['sum', 'mean'],\n",
    "            'Quantity': ['sum', 'mean'],\n",
    "            'ProductID': 'nunique'\n",
    "    }).reset_index()\n",
    "        \n",
    "    customer_metrics.columns = ['CustomerID', 'transaction_count', 'total_value', \n",
    "                                'avg_value', 'total_quantity', 'avg_quantity', \n",
    "                                'unique_products']\n",
    "        \n",
    "    pur_dates = transactions.groupby('CustomerID').agg({\n",
    "            'TransactionDate': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    pur_dates.columns = ['CustomerID', 'first_purchase', 'last_purchase']\n",
    "        \n",
    "    cur_date = transactions['TransactionDate'].max()\n",
    "    pur_dates['days_since_first'] = (cur_date - pd.to_datetime(pur_dates['first_purchase'])).dt.days\n",
    "    pur_dates['days_since_last'] = (cur_date - pd.to_datetime(pur_dates['last_purchase'])).dt.days\n",
    "        \n",
    "    customer_features = customers.merge(customer_metrics, on='CustomerID', how='left')\n",
    "    customer_features = customer_features.merge(pur_dates, on='CustomerID', how='left')\n",
    "        \n",
    "    cat_preferences = transactions.merge(products[['ProductID', 'Category']], on='ProductID')\\\n",
    "            .groupby(['CustomerID', 'Category'])['TotalValue'].sum().unstack(fill_value=0)\n",
    "        \n",
    "    cat_preferences = cat_preferences.div(cat_preferences.sum(axis=1), axis=0)\n",
    "        \n",
    "    final_features = customer_features.merge(cat_preferences, on='CustomerID', how='left')\n",
    "    final_features = final_features.fillna(0)\n",
    "        \n",
    "    return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df, exclude_cols):\n",
    "    scaler = StandardScaler()\n",
    "    df_normalized = df.copy()\n",
    "        \n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    num_cols = [col for col in num_cols if col not in exclude_cols]\n",
    "        \n",
    "    df_normalized[num_cols] = scaler.fit_transform(df_normalized[num_cols])\n",
    "    return df_normalized, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_clusters=10):\n",
    "    db_scores = []\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(data)\n",
    "        score = davies_bouldin_score(data, clusters)\n",
    "        db_scores.append(score)\n",
    "        \n",
    "    optimal_clusters = np.argmin(db_scores) + 2\n",
    "    return optimal_clusters, db_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(data, clusters, save_path=None):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_2d = pca.fit_transform(data)\n",
    "        \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(data_2d[:, 0], data_2d[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('customer Segments')\n",
    "    plt.xlabel('1st principal component')\n",
    "    plt.ylabel('2nd principal component')\n",
    "        \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_profiles(original_data, clusters):\n",
    "    data_with_clusters = original_data.copy()\n",
    "    data_with_clusters['Cluster'] = clusters\n",
    "        \n",
    "    profiles = []\n",
    "    for cluster in range(len(np.unique(clusters))):\n",
    "        cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]\n",
    "        profile = {\n",
    "                'Cluster': cluster,\n",
    "                'Size': len(cluster_data),\n",
    "                'Avg_Transaction_Value': cluster_data['avg_value'].mean(),\n",
    "                'Avg_Purchase_Frequency': cluster_data['transaction_count'].mean(),\n",
    "                'Avg_Recency': cluster_data['days_since_last'].mean(),\n",
    "                'Total_Revenue': cluster_data['total_value'].sum(),\n",
    "                'Avg_Products_Purchased': cluster_data['unique_products'].mean()\n",
    "        }\n",
    "        profiles.append(profile)\n",
    "        \n",
    "    return pd.DataFrame(profiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully!\n",
      "Creating customer features...\n",
      "Normalizing features...\n",
      "Finding optimal number of clusters...\n",
      "Performing clustering with 8 clusters...\n",
      "DB Index: 1.5823\n",
      "generating visualization.\n",
      "customer segmentation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    output_path = \"clustering_results\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        customers, transactions, products = load_data()\n",
    "        \n",
    "        print(\"Creating customer features...\")\n",
    "        customer_features = create_customer_features(customers, transactions, products)\n",
    "        \n",
    "        exclude_cols = ['CustomerID', 'CustomerName', 'Region', 'SignupDate', \n",
    "                    'first_purchase', 'last_purchase']\n",
    "        \n",
    "        cluster_features = customer_features.drop(exclude_cols, axis=1)\n",
    "        \n",
    "        print(\"Normalizing features...\")\n",
    "        features_normalized, scaler = normalize_features(cluster_features, ['CustomerID'])\n",
    "        \n",
    "        print(\"Finding optimal number of clusters...\")\n",
    "        optimal_clusters, db_scores = find_optimal_clusters(features_normalized)\n",
    "        \n",
    "        print(f\"Performing clustering with {optimal_clusters} clusters...\")\n",
    "        kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(features_normalized)\n",
    "        \n",
    "        db_index = davies_bouldin_score(features_normalized, clusters)\n",
    "        print(f\"DB Index: {db_index:.4f}\")\n",
    "        \n",
    "        print(\"generating visualization.\")\n",
    "        visualize_clusters(features_normalized, clusters, f\"{output_path}/cluster_visualization.png\")\n",
    "        \n",
    "        cluster_profiles = generate_cluster_profiles(customer_features, clusters)\n",
    "        customer_clusters = pd.DataFrame({\n",
    "            'CustomerID': customer_features['CustomerID'],\n",
    "            'Cluster': clusters\n",
    "        })\n",
    "        \n",
    "        cluster_profiles.to_csv(f\"{output_path}/cluster_profiles.csv\", index=False)\n",
    "        customer_clusters.to_csv(f\"{output_path}/customer_clusters.csv\", index=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(2, len(db_scores) + 2), db_scores, marker='o')\n",
    "        plt.xlabel('No. of Clusters')\n",
    "        plt.ylabel('DB Index')\n",
    "        plt.title('DB Index vs No. of Clusters')\n",
    "        plt.savefig(f\"{output_path}/db_scores.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"customer segmentation completed successfully!\")\n",
    "        return {\n",
    "            'optimal_clusters': optimal_clusters,\n",
    "            'db_index': db_index,\n",
    "            'cluster_profiles': cluster_profiles,\n",
    "            'customer_clusters': customer_clusters\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
